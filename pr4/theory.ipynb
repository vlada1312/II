{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинг — это представление объектов, таких как слова, в виде векторов чисел. Эти векторы захватывают семантические и синтаксические характеристики объектов, что позволяет моделям машинного обучения работать с ними более эффективно. т.е. можно считить что эмбеддинг - это вектор  \n",
    "\n",
    "---\n",
    "\n",
    "Слой эмбеддинга в модели машинного обучения — это специальный слой, который преобразует идентификаторы (индексы) в векторы эмбеддингов. Это как словарь, где каждому слову соответствует его векторное представление.\n",
    "\n",
    "---\n",
    "\n",
    "Входной и выходной вокабуляр — это списки всех слов или токенов, которые модель может принимать как входные данные и выводить как результаты. В контексте машинного перевода, входной вокабуляр может содержать слова на одном языке, а выходной — на другом.\n",
    "\n",
    "---\n",
    "\n",
    "Энкодер в модели seq2seq принимает входные данные и преобразует их в набор внутренних представлений, которые захватывают ключевую информацию из этих данных. Декодер затем использует эту информацию для генерации выходных данных, например, перевода текста на другой язык.\n",
    "\n",
    "---\n",
    "\n",
    "Скрытое (внутреннее) состояние декодера — это внутренние переменные декодера, которые он использует для хранения информации о том, что было обработано до текущего момента. Эти состояния позволяют декодеру учитывать контекст при генерации каждого нового элемента выходных данных.\n",
    "\n",
    "---\n",
    "\n",
    "Батч — это набор данных, который обрабатывается моделью одновременно. Вместо того, чтобы обучаться на одном примере за раз, модели машинного обучения часто обрабатывают батчи данных для более эффективного обучения.\n",
    "\n",
    "---\n",
    "\n",
    "Паддинг — это добавление специальных символов (обычно нулей или специальных токенов) к данным, чтобы сделать их размер одинаковым. Это необходимо, потому что многие алгоритмы машинного обучения требуют, чтобы все входные данные были одного размера для параллельной обработки.\n",
    "\n",
    "---\n",
    "\n",
    "Логиты представляют собой выходные данные нейронной сети, которые перед применением функции активации, такой как softmax, являются необработанными предсказаниями. Логиты — это векторы, каждый элемент которых соответствует \"сырой\" оценке вероятности того, что входной образец принадлежит к определенному классу или категории.\n",
    "\n",
    "    1. В контексте задачи машинного перевода или другой задачи генерации текста, логиты — это выходные данные последнего слоя нейронной сети (обычно линейного слоя), представляющие предварительные вероятности для каждого возможного следующего слова или символа в последовательности.\n",
    "    2. Каждый элемент в векторе логитов соответствует вероятности того, что конкретное слово из словаря будет следующим словом в предложении.\n",
    "    3. Функция softmax преобразует эти логиты в нормализованные вероятности, которые используются для выбора наиболее вероятного слова при генерации текста.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "GRU (Gated Recurrent Unit) — это тип рекуррентной нейронной сети, которая эффективно обрабатывает последовательности данных, например, текст или временные ряды. GRU помогает модели запоминать важную информацию из прошлых данных и забывать то, что уже не актуально, благодаря своей структуре с \"воротами\".\n",
    "\n",
    "---\n",
    "\n",
    "One-hot вектор — это способ представления категориальных переменных в виде бинарного вектора в машинном обучении и статистике. В one-hot векторе:\n",
    "\n",
    "    1. Все элементы вектора равны 0, за исключением одного, который равен 1.\n",
    "    2. Индекс элемента, который установлен в 1, соответствует категории или классу, которому принадлежит образец.\n",
    "\n",
    "Пример: Предположим, у нас есть вокабуляр из пяти слов: [\"apple\", \"banana\", \"cherry\", \"date\", \"fig\"]. Для представления слова \"banana\" в формате one-hot вектора мы используем вектор [0, 1, 0, 0, 0], где второй элемент (с индексом 1) установлен в 1, что указывает на то, что текущее слово — \"banana\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) — это алгоритм сжатия данных, который также нашёл широкое применение в обработке естественного языка, особенно в задачах, связанных с машинным переводом и моделированием языка. Суть BPE заключается в создании эффективного фиксированного словаря для больших объёмов текстовых данных.\n",
    "\n",
    "### Как работает BPE\n",
    "BPE начинает с токенизации текста на базовые символы (например, отдельные буквы или знаки пунктуации) и последовательно объединяет наиболее часто встречающиеся пары символов в новые, более длинные токены. Этот процесс повторяется многократно в соответствии с заданным количеством итераций или до достижения указанного размера словаря.\n",
    "\n",
    "### Пример работы BPE\n",
    "Представим, что у нас есть текст: \"aaabdaaabac\". Если применить BPE к этому тексту, мы можем начать с нахождения наиболее часто встречающихся пар символов. В нашем случае это \"aa\". Заменяем каждое вхождение \"aa\" на новый символ, например, \"z\". Теперь наш текст выглядит как \"zabdazabac\". Процесс повторяется, и мы можем далее объединять другие часто встречающиеся пары.\n",
    "\n",
    "### Правила BPE\n",
    "Правила BPE, которые генерируются в процессе обучения алгоритма на текстовом корпусе, определяют, какие именно пары символов или подслов соединять. Эти правила сохраняются и используются для последующей токенизации новых текстов, гарантируя, что токенизация будет выполнена консистентно, что критически важно для обучения и использования моделей машинного обучения.\n",
    "\n",
    "BPE стал основой для более новых методов токенизации, таких как SentencePiece или токенизаторы, используемые в моделях Transformer, таких как BERT или GPT, где он помогает улучшить обработку текстов на различных языках и в разнообразных задачах.\n",
    "\n",
    "### word dropout\n",
    "\n",
    "Word dropout — это техника, используемая в обработке естественного языка, чтобы сделать модели более устойчивыми к различным входным данным и предотвратить их переобучение. Суть этой техники заключается в случайном заменении части слов в входном тексте на специальный токен, обычно называемый unk, который представляет неизвестное или отсутствующее слово.\n",
    "\n",
    "Для чего?\n",
    "\n",
    "1) Заменяя часть слов на unk, модель обучается не полагаться слишком сильно на любое конкретное слово или набор слов, которые могут быть часто встречающимися в обучающем наборе данных, но редко появляться в новых, неизвестных текстах. Это помогает улучшить способность модели обобщать знания на новые данные, что критически важно для успешной работы модели в реальных условиях.\n",
    "\n",
    "2) Токен unk может представлять не только неизвестные слова, но и слова с ошибками или опечатками, которые не встречаются в обучающем словаре. Обучение модели на том, чтобы иногда игнорировать конкретные слова и заменять их на unk, помогает ей лучше справляться с неидеальными или зашумленными входными данными.\n",
    "\n",
    "3) Когда модель слишком хорошо \"запоминает\" тренировочные данные, она может стать менее эффективной на новых данных — явление, известное как переобучение. Использование unk препятствует этому, поскольку модель не может полагаться на точное знание каждого слова и вынуждена изучать более обширные и устойчивые паттерны в данных.\n",
    "\n",
    "4) В реальных условиях модели могут столкнуться с огромными объемами слов, многие из которых редко используются. Замена на unk позволяет сократить размер словаря и сделать модель более легкой и быстрой, поскольку уменьшается количество параметров, необходимых для обучения и хранения информации о каждом уникальном слове.\n",
    "\n",
    "### scheduling\n",
    "\n",
    "Термин \"scheduling\" в контексте нейросетей обычно относится к постепенному изменению некоторого параметра в процессе обучения.\"scheduled dropout\" означает, что вероятность применения dropout к элементам входных данных изменяется по заданному расписанию.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
